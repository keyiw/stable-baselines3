diff --git a/stable_baselines3/common/torch_layers.py b/stable_baselines3/common/torch_layers.py
index 234b915..229f705 100644
--- a/stable_baselines3/common/torch_layers.py
+++ b/stable_baselines3/common/torch_layers.py
@@ -4,6 +4,7 @@ import gymnasium as gym
 import torch as th
 from gymnasium import spaces
 from torch import nn
+import sys
 
 from stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space
 from stable_baselines3.common.type_aliases import TensorDict
diff --git a/stable_baselines3/common/vec_env/dummy_vec_env.py b/stable_baselines3/common/vec_env/dummy_vec_env.py
index 15ecfb6..8e8978d 100644
--- a/stable_baselines3/common/vec_env/dummy_vec_env.py
+++ b/stable_baselines3/common/vec_env/dummy_vec_env.py
@@ -71,14 +71,20 @@ class DummyVecEnv(VecEnv):
             self._save_obs(env_idx, obs)
         return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones), deepcopy(self.buf_infos))
 
+    # def reset(self) -> VecEnvObs:
+    #     for env_idx in range(self.num_envs):
+    #         maybe_options = {"options": self._options[env_idx]} if self._options[env_idx] else {}
+    #         obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
+    #         self._save_obs(env_idx, obs)
+    #     # Seeds and options are only used once
+    #     self._reset_seeds()
+    #     self._reset_options()
+    #     return self._obs_from_buf()
+    
     def reset(self) -> VecEnvObs:
         for env_idx in range(self.num_envs):
-            maybe_options = {"options": self._options[env_idx]} if self._options[env_idx] else {}
-            obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
+            obs = self.envs[env_idx].reset()
             self._save_obs(env_idx, obs)
-        # Seeds and options are only used once
-        self._reset_seeds()
-        self._reset_options()
         return self._obs_from_buf()
 
     def close(self) -> None:
diff --git a/stable_baselines3/common/vec_env/vec_video_recorder.py b/stable_baselines3/common/vec_env/vec_video_recorder.py
index 52faebd..e49f425 100644
--- a/stable_baselines3/common/vec_env/vec_video_recorder.py
+++ b/stable_baselines3/common/vec_env/vec_video_recorder.py
@@ -1,7 +1,8 @@
 import os
 from typing import Callable
 
-from gymnasium.wrappers.monitoring import video_recorder
+# from gymnasium.wrappers.monitoring import video_recorder
+from gym.wrappers.monitoring import video_recorder
 
 from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvObs, VecEnvStepReturn, VecEnvWrapper
 from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv
diff --git a/stable_baselines3/ppo/ppo.py b/stable_baselines3/ppo/ppo.py
index 52ee2eb..de8fbec 100644
--- a/stable_baselines3/ppo/ppo.py
+++ b/stable_baselines3/ppo/ppo.py
@@ -14,6 +14,22 @@ from stable_baselines3.common.utils import explained_variance, get_schedule_fn
 
 SelfPPO = TypeVar("SelfPPO", bound="PPO")
 
+class AdaptiveScheduler:
+    def __init__(self, kl_threshold, min_lr, max_lr, init_lr):
+        super().__init__()
+        self.min_lr = min_lr
+        self.max_lr = max_lr
+        self.kl_threshold = kl_threshold
+        self.current_lr = init_lr
+
+    def update(self, kl_dist):
+        lr = self.current_lr
+        if kl_dist > (2.0 * self.kl_threshold):
+            lr = max(self.current_lr / 1.5, self.min_lr)
+        if kl_dist < (0.5 * self.kl_threshold):
+            lr = min(self.current_lr * 1.5, self.max_lr)
+        self.current_lr = lr
+        return lr
 
 class PPO(OnPolicyAlgorithm):
     """
@@ -105,6 +121,9 @@ class PPO(OnPolicyAlgorithm):
         seed: Optional[int] = None,
         device: Union[th.device, str] = "auto",
         _init_setup_model: bool = True,
+        adaptive_kl: float = 0.02,  
+        min_lr=1e-4,
+        max_lr=1e-3,
     ):
         super().__init__(
             policy,
@@ -166,6 +185,8 @@ class PPO(OnPolicyAlgorithm):
         self.clip_range_vf = clip_range_vf
         self.normalize_advantage = normalize_advantage
         self.target_kl = target_kl
+        self.kl_scheduler = AdaptiveScheduler(kl_threshold=adaptive_kl, min_lr=min_lr, max_lr=max_lr,
+                                              init_lr=learning_rate)
 
         if _init_setup_model:
             self._setup_model()
