diff --git a/stable_baselines3/common/torch_layers.py b/stable_baselines3/common/torch_layers.py
index 234b915..229f705 100644
--- a/stable_baselines3/common/torch_layers.py
+++ b/stable_baselines3/common/torch_layers.py
@@ -4,6 +4,7 @@ import gymnasium as gym
 import torch as th
 from gymnasium import spaces
 from torch import nn
+import sys
 
 from stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space
 from stable_baselines3.common.type_aliases import TensorDict
diff --git a/stable_baselines3/ppo/ppo.py b/stable_baselines3/ppo/ppo.py
index 52ee2eb..de8fbec 100644
--- a/stable_baselines3/ppo/ppo.py
+++ b/stable_baselines3/ppo/ppo.py
@@ -14,6 +14,22 @@ from stable_baselines3.common.utils import explained_variance, get_schedule_fn
 
 SelfPPO = TypeVar("SelfPPO", bound="PPO")
 
+class AdaptiveScheduler:
+    def __init__(self, kl_threshold, min_lr, max_lr, init_lr):
+        super().__init__()
+        self.min_lr = min_lr
+        self.max_lr = max_lr
+        self.kl_threshold = kl_threshold
+        self.current_lr = init_lr
+
+    def update(self, kl_dist):
+        lr = self.current_lr
+        if kl_dist > (2.0 * self.kl_threshold):
+            lr = max(self.current_lr / 1.5, self.min_lr)
+        if kl_dist < (0.5 * self.kl_threshold):
+            lr = min(self.current_lr * 1.5, self.max_lr)
+        self.current_lr = lr
+        return lr
 
 class PPO(OnPolicyAlgorithm):
     """
@@ -105,6 +121,9 @@ class PPO(OnPolicyAlgorithm):
         seed: Optional[int] = None,
         device: Union[th.device, str] = "auto",
         _init_setup_model: bool = True,
+        adaptive_kl: float = 0.02,  
+        min_lr=1e-4,
+        max_lr=1e-3,
     ):
         super().__init__(
             policy,
@@ -166,6 +185,8 @@ class PPO(OnPolicyAlgorithm):
         self.clip_range_vf = clip_range_vf
         self.normalize_advantage = normalize_advantage
         self.target_kl = target_kl
+        self.kl_scheduler = AdaptiveScheduler(kl_threshold=adaptive_kl, min_lr=min_lr, max_lr=max_lr,
+                                              init_lr=learning_rate)
 
         if _init_setup_model:
             self._setup_model()
